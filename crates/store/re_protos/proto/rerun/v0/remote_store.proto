syntax = "proto3";

package rerun.remote_store.v0;

import "rerun/v0/common.proto";

service StorageNode {
    // data API calls
    rpc Query(QueryRequest) returns (stream DataframePart) {}
    rpc FetchRecording(FetchRecordingRequest) returns (stream rerun.common.v0.RerunChunk) {}

    rpc IndexCollection(IndexCollectionRequest) returns (IndexCollectionResponse) {}
    // Collection index query response is a RecordBatch with 2 columns:
    // - 'resource_id' column with the id of the resource
    // - timepoint column with the values reprensenting the points in time
    // where index query matches. What time points are matched depends on the type of
    // index that is queried. For example for vector search it might be timepoints where
    // top-K matches are found within *each* resource in the collection. For inverted index
    // it might be timepoints where the query string is found in the indexed column
    rpc QueryCollectionIndex(QueryCollectionIndexRequest) returns (stream DataframePart) {}

    // metadata API calls
    rpc QueryCatalog(QueryCatalogRequest) returns (stream DataframePart) {}
    rpc UpdateCatalog(UpdateCatalogRequest) returns (UpdateCatalogResponse) {}
    rpc GetRecordingSchema(GetRecordingSchemaRequest) returns (GetRecordingSchemaResponse) {}

    // TODO(zehiko) support registering more than one recording at a time
    rpc RegisterRecording(RegisterRecordingRequest) returns (DataframePart) {}

    rpc UnregisterRecording(UnregisterRecordingRequest) returns (UnregisterRecordingResponse) {}
    rpc UnregisterAllRecordings(UnregisterAllRecordingsRequest) returns (UnregisterAllRecordingsResponse) {}
}

// ---------------- Common response message ------------------

// DataframePart is arrow IPC encoded RecordBatch
message DataframePart {
    // encoder version used to encode the data
    rerun.common.v0.EncoderVersion encoder_version = 1;

    // Data payload is Arrow IPC encoded RecordBatch
    bytes payload = 1000;
}

// ---------------- IndexCollection ------------------

message IndexCollectionRequest {
    // which collection do we want to create index for
    Collection collection = 1;
    // what kind of index do we want to create and what are
    // its index specific properties
    IndexType index_type = 2;
    // Component / column we want to index
    rerun.common.v0.ComponentColumnDescriptor column = 3;
    // What is the filter index i.e. timeline for which we
    // will query the timepoints
    rerun.common.v0.IndexColumnSelector time_index = 4;
}

message IndexType {
    oneof typ {
        InvertedIndex inverted = 1;
        VectorIvfPqIndex vector = 2;
        BTreeIndex btree = 3;
    }
}

message InvertedIndex {
    bool store_position = 1;
    string base_tokenizer  = 2;
    // TODO(zehiko) add properties as needed
}

message VectorIvfPqIndex {
    uint32 num_partitions = 1;
    uint32 num_sub_vectors = 2;
    VectorDistanceMetric distance_metrics = 3;
}

enum VectorDistanceMetric {
    L2 = 0;
    COSINE = 1;
    DOT = 2;
    HAMMING = 3;
}

message BTreeIndex {

}

message IndexCollectionResponse {}


// ---------------- QueryCollectionIndex ------------------

message QueryCollectionIndexRequest {
    // Collection we want to run the query against on
    // If not specified, the default collection is queried
    Collection collection = 1;
    // Index type specific query properties
    IndexQueryData index = 2;
}

message IndexQueryData {
    // specific index properties based on the index type
    oneof index_query {
        InvertedIndexQuery inverted = 1;
        VectorIndexQuery vector = 2;
        BTreeIndexQuery btree = 3;
    }
}

message InvertedIndexQuery {
    // Query to execute represented as the arrow data
    // Query should be a unit RecordBatch with 2 columns:
    // - 'index' column with the name of the column we want to query
    // - 'query' column with the value we want to query. It must be
    // of utf8 type
    DataframePart query = 1;
    // TODO(zehiko) add properties as needed
}

message VectorIndexQuery {
    // Query to execute represented as the arrow data
    // Query should be a unit RecordBatch with 2 columns:
    // - 'index' column with the name of the column we want to query
    // - 'query' column with the value we want to query. It must be of
    // type of float32 array
    DataframePart query = 1;
    uint32 top_k = 2;
}

message BTreeIndexQuery {
    // Query to execute represented as the arrow data
    // Query should be a unit RecordBatch with 2 columns:
    // - 'index' column with the name of the column we want to query
    // - 'query' column with the value we want to query. The type should
    // be of the same type as the indexed column
    DataframePart query = 1;
    // TODO(zehiko) add properties as needed
}

message Collection {
    string name = 1;
}

// ---------------- GetRecordingSchema ------------------

message GetRecordingSchemaRequest {
    rerun.common.v0.RecordingId recording_id = 1;
}

message GetRecordingSchemaResponse {
    repeated rerun.common.v0.ColumnDescriptor column_descriptors = 1;
}

// ---------------- RegisterRecording ------------------

message RegisterRecordingRequest {
    // human readable description of the recording
    string description = 1;
    // recording storage url (e.g. s3://bucket/file or file:///path/to/file)
    string storage_url = 2;
    // type of recording
    RecordingType typ = 3;
    // (optional) any additional metadata that should be associated with the recording
    // You can associate any arbtrirary number of columns with a specific recording
    DataframePart metadata = 4;
}

// ---------------- Unregister from catalog ------------------

message UnregisterRecordingRequest {
    // unique identifier of the recording
    rerun.common.v0.RecordingId recording_id = 1;
}
message UnregisterRecordingResponse {}

message UnregisterAllRecordingsRequest {}
message UnregisterAllRecordingsResponse {}


// ---------------- UpdateCatalog  -----------------

message UpdateCatalogRequest {
    DataframePart metadata = 2;
}

message UpdateCatalogResponse {}

// ---------------- Query -----------------

message QueryRequest {
    // unique identifier of the recording
    rerun.common.v0.RecordingId recording_id = 1;
    // query to execute
    rerun.common.v0.Query query = 3;
}

// ----------------- QueryCatalog -----------------

message QueryCatalogRequest {
    // Column projection - define which columns should be returned.
    // Providing it is optional, if not provided, all columns should be returned
    ColumnProjection column_projection = 1;
    // Filter specific recordings that match the criteria (selection)
    CatalogFilter filter = 2;
}

message ColumnProjection {
    repeated string columns = 1;
}

message CatalogFilter {
    // Filtering is very simple right now, we can only select
    // recordings by their ids.
    repeated rerun.common.v0.RecordingId recording_ids = 1;
}

message QueryCatalogResponse {
    rerun.common.v0.EncoderVersion encoder_version = 1;
    // raw bytes are TransportChunks (i.e. RecordBatches) encoded with the relevant codec
    bytes payload = 2;
}

enum RecordingType {
    RRD = 0;
}

// ----------------- FetchRecording -----------------

message FetchRecordingRequest {
    rerun.common.v0.RecordingId recording_id = 1;
}

// TODO(jleibs): Eventually this becomes either query-mediated in some way, but for now
// it's useful to be able to just get back the whole RRD somehow.
message FetchRecordingResponse {
    // TODO(zehiko) we need to expand this to become something like 'encoder options'
    // as we will need to specify additional options like compression, including schema
    // in payload, etc.
    rerun.common.v0.EncoderVersion encoder_version = 1;
    // payload is raw bytes that the relevant codec can interpret
    bytes payload = 2;
}

// ----------------- Error handling -----------------

// Application level error - used as `details` in the `google.rpc.Status` message
message RemoteStoreError {
    // error code
    ErrorCode code = 1;
    // unique identifier associated with the request (e.g. recording id, recording storage url)
    string id = 2;
    // human readable details about the error
    string message = 3;
}

// Error codes for application level errors
enum ErrorCode {
    // unused
    _UNUSED = 0;

    // object store access error
    OBJECT_STORE_ERROR = 1;

    // metadata database access error
    METADATA_DB_ERROR = 2;

    // Encoding / decoding error
    CODEC_ERROR = 3;
}
